# Experiment configuration file.
#
# There are two special blocks. The 'seml' block is required for every experiment.
# It has to contain the following values:
# db_collection: Name of the MongoDB collection to save the experiment information to
# executable:    Name of the Python script containing the experiment
# It can optionally also contain the following values:
# conda_environment: Specifies which conda environment will be activated before the experiment is executed.
# output_dir:    Directory to store log files in. Default: Current directory
#
# The special 'slurm' block contains the slurm parameters. This block and all values are optional. Possible values are:
# name:                 Job name used by Slurm and file name of Slurm output. Default: Collection name
# experiments_per_job:  Number of parallel experiments to run in each Slurm job.
#                       Note that only experiments from the same batch share a job. Default: 1
# max_jobs_per_batch:   Maximum number of Slurm jobs running per experiment batch. Default: No restriction
# sbatch_options:       dictionary that contains custom values that will be passed to `sbatch`, specifying e.g.
#                       the memory and number of GPUs to be allocated (prepended dashes are not required). See
#                       https://slurm.schedmd.com/sbatch.html for all possible options.
#
# Parameters under 'fixed' will be used for all the experiments.
#
# Under 'grid' you can define parameters that should be sampled from a regular grid. Options are:
#   - choice:     List the different values you want to evaluate under 'choices' as in the example below.
#   - range:      Specify the min, max, and step. Parameter values will be generated using np.arange(min, max, step).
#   - uniform:    Specify the min, max, and num. Parameter values will be generated using
#                 np.linspace(min, max, num, endpoint=True)
#   - loguniform: Specify min, max, and num. Parameter values will be uniformly generated in log space (base 10).
#
# Under 'random' you can specify parameters for which you want to try several random values. Specify the number
# of samples per parameter with the 'samples' value as in the examples below.
# Specify the the seed under the 'random' dict or directly for the desired parameter(s).
# Supported parameter types are:
#   - choice:      Randomly samples <samples> entries (with replacement) from the list in parameter['options']
#   - uniform:     Uniformly samples between 'min' and 'max' as specified in the parameter dict.
#   - loguniform:  Uniformly samples in log space between 'min' and 'max' as specified in the parameter dict.
#   - randint:     Randomly samples integers between 'min' (included) and 'max' (excluded).
#
# The configuration file can be nested (as the example below) so that we can run different parameter sets
# e.g. for different datasets or models.
# We take the cartesian product of all `grid` parameters on a path and sample all random parameters on the path.
# The number of random parameters sampled will be max{n_samples} of all n_samples on the path. This is done because
# we need the same number of samples from all random parameters in a configuration.
#
# More specific settings (i.e., further down the hierarchy) always overwrite more general ones.


seml:
  db_collection: check_fails
  executable: experiment_check_fails.py
  output_dir: slurm

slurm:
  name: check_fails
  experiments_per_job: 1
  sbatch_options:
    gres: gpu:1       # num GPUs
    mem: 16G          # memory
    cpus-per-task: 1  # num cores
    time: 0-08:00     # max time, D-HH:MM

###### BEGIN PARAMETER CONFIGURATION ######

fixed:
  # comment out these two lines and the check succeeds
  observe_slack: True
  observe_neptune: True

  model:
    layer:
      size: 100
    activation: relu
  optimizer:
    opt: adam
    lr: 1e-3
  dataset: my_dataset

